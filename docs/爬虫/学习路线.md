# 爬虫学习路线
:::tip 路线从哪里来的？
哪有什么学习路线？
不过是大佬们水群吹牛的附带产物罢了~

大佬每水群一句，我就copy一句，于是有了这篇学习路线
:::
:::tip 《吹》
我把大佬们的聊天记录反复观看，仔细揣摩

歪歪扭扭，话外有话，

怎么面试？

吹就完了
:::



## 待整理

### dp【DrissionPage】

DP是基于cdp的，学习cdp可以看官方文档，配合沉浸式翻译。

https://chromedevtools.github.io/devtools-protocol/

dp目前没有检测办法吧，就相当于手动控制台调试

硬要检测的话，我觉得不是不可以，cdp协议本身没有鉴权，只要开了就能直接访问到



指纹检测网站：

https://www.browserscan.net/



frida hook原理 Xposed的hook原理.

in line hook 是干啥的.

搞了什么App 干了什么js 

App 现在有什么常用的 检测方案 反调试方案 

ptrace
TracerPid
特征检测  什么几个 frida 关键字啊  端口啊

然后常用的几个汇编指令   B指令 add治理  bl指令 BX指令
然后吹一吹 unidbg 是吧.
ida的快捷键

# js
问js  就吹一下 ast啊

然后看看vpm啊

分布式爬虫啊..

日采集 几百万 几千万数据什么的.

阿卡麦 PX 这种的吧.

5S   阿卡麦 3年前就写成纯C代码了

点选验证码的地址有时候是点选，有时候会变成旋转，估计是异常等级不同导致的

浏览器指纹，指纹浏览器

安卓逆向面试题汇总 技术篇
面试官经常问的几个问题如下：

常见的加固手段有哪些
安卓反调试一般有哪些手段，怎么去防范
arm汇编 b bl bx blx 这些指令是什么意思
ida xx操作的快捷键是哪个？
Xposed hook 原理 frida hook 原理
inline hook原理
ollvm 代码混淆你了解吗？要怎么去处理





# JS逆向目录

::: tip

下面的所有技术，名词，你最好要深入思考，为什么要有这项技术，它的好处是什么，想要解决什么问题？又要如何应对,你会应对吗.

:::



## 前言





## 科普



### 所见即所得





## 基础环境搭建



## JS/NodeJs



## 前端

#### url编码

URL编码（也称为百分号编码或URL转义）是一种用于在URL中表示特殊字符和非ASCII字符的编码方式。由于URL只允许特定的字符，因此如果要包含特殊字符或非ASCII字符（例如空格、问号、斜杠、中文字符等），就需要对其进行编码，以便在URL中安全地传输和解析。

URL编码的原理是将要编码的字符转换为 `%` 符号后跟两位十六进制数的形式。例如，空格字符会被编码为 `%20`，问号字符会被编码为 `%3F`，而中文字符会被分解为其UTF-8编码的字节序列，然后每个字节以 `%` 符号后跟两位十六进制数的形式进行编码。

例如，假设我们要将字符串 "Hello World?" 编码为URL编码形式，那么编码后的结果将会是 "Hello%20World%3F"。

URL编码通常使用在Web开发中，例如在GET请求中传递参数时，或者在构建动态URL时。在Python中，可以使用`urllib.parse`模块进行URL编码和解码的操作。

示例：

```python
from urllib.parse import quote, unquote

original_url = "https://www.example.com/search?q=hello world?"

# URL编码
encoded_url = quote(original_url)
print("Encoded URL:", encoded_url)  # 输出：https%3A//www.example.com/search%3Fq%3Dhello%20world%3F

# URL解码
decoded_url = unquote(encoded_url)
print("Decoded URL:", decoded_url)  # 输出：https://www.example.com/search?q=hello world?
```

这个示例演示了如何使用Python中的`urllib.parse.quote()`函数对URL进行编码，以及使用`urllib.parse.unquote()`函数对URL进行解码。

```javascript
var originalUrl = "https://www.example.com/search?q=hello world?";
var encodedUrl = encodeURIComponent(originalUrl);
console.log("Encoded URL:", encodedUrl);
// 输出：https%3A%2F%2Fwww.example.com%2Fsearch%3Fq%3Dhello%20world%3F
```

```javascript
var encodedUrl = "https%3A%2F%2Fwww.example.com%2Fsearch%3Fq%3Dhello%20world%3F";
var decodedUrl = decodeURIComponent(encodedUrl);
console.log("Decoded URL:", decodedUrl);
// 输出：https://www.example.com/search?q=hello world?
```

url编码

## 数据库



### mysql

### mongodb







#### 自执行函数





#### base64编码
##### 概念
Base64是一种用于将二进制数据编码为文本的编码方式。它的名称来源于它使用的字符集，该字符集由64个字符组成，包括大写字母、小写字母、数字和两个额外的符号。
##### 组成
Base64编码使用的字符集包括：

1. 大写字母 A-Z（共26个字符）
2. 小写字母 a-z（共26个字符）
3. 数字 0-9（共10个字符）
4. 加号 "+" 和斜杠 "/"（共2个字符）

通常情况下，Base64编码还会使用一个额外的符号来表示填充（padding），通常是等号 "="。这些字符共同构成了Base64编码所用的64个字符。
##### 用途
Base64编码通常用于在网络传输或存储中表示二进制数据，例如在电子邮件附件中传输二进制文件或在URL中传输数据。

Base64编码通过将3个字节的二进制数据编码为4个ASCII字符，因此它的编码长度总是4的倍数。如果原始数据的长度不是3的倍数，Base64编码器将在末尾添加一个或两个额外的字符（通常是等号），以使编码后的字符串长度成为4的倍数。

当将3个字节的二进制数据编码为4个ASCII字符时，Base64编码器会将这3个字节的数据划分为四组，每组6个比特。然后，每个6比特的值将被映射到Base64字符集中的相应字符。这样，每组6比特将被映射为一个Base64字符。

例如，假设我们有这样的3个字节的二进制数据：01011010 11001101 10101000。将其分成四组6比特，得到：

010110 (二进制为 22，对应Base64字符集中的字符为 'W')
101101 (二进制为 45，对应Base64字符集中的字符为 't')
101101 (二进制为 45，对应Base64字符集中的字符为 't')
010110 (二进制为 22，对应Base64字符集中的字符为 'W')
然后，将这四个Base64字符连接起来，就得到了最终的Base64编码字符串，即 "WttW"。

总结来说，将3个字节的二进制数据编码为4个ASCII字符的过程是将原始数据划分为4组6比特，然后根据这些6比特值在Base64字符集中找到对应的字符。

学习了base64，为什么是三个字节转化为4个字符，因为一个字节八位，64个字符可以代表6位，八和六的最小公倍数是24。妙啊！！！

Base64编码的主要特点是它可以将二进制数据转换为纯文本形式，这使得它在许多不支持二进制数据传输的环境中非常有用。它也是许多网络协议和数据存储格式的一部分，如HTTP、SMTP、JSON等。

Base64编码虽然能够将二进制数据转换为文本，但它并不是加密算法，因为Base64编码可以被轻松地解码还原为原始的二进制数据。因此，不应该将Base64编码用于敏感数据的加密或安全传输。

在浏览器中和Python中都可以使用内置的函数或库来进行Base64编码和解码操作。

在浏览器中，可以使用JavaScript的内置函数`btoa()`进行Base64编码，以及`atob()`进行解码。

示例：

```javascript
// Base64编码
var encodedData = btoa("Hello, world!");
console.log(encodedData); // 输出： "SGVsbG8sIHdvcmxkIQ=="

// Base64解码
var decodedData = atob(encodedData);
console.log(decodedData); // 输出： "Hello, world!"
```

在Python中，可以使用内置的`base64`模块进行Base64编码和解码操作。

示例：

```python
import base64

# Base64编码
encoded_data = base64.b64encode(b"Hello, world!")
print(encoded_data)  # 输出： b'SGVsbG8sIHdvcmxkIQ=='

# Base64解码
decoded_data = base64.b64decode(encoded_data)
print(decoded_data.decode("utf-8"))  # 输出： "Hello, world!"
```

这些示例演示了在浏览器中使用JavaScript和在Python中使用`base64`模块进行Base64编码和解码的方法。

一般情况下，图片通常会被转换成Base64编码形式。这种转换将图片的二进制数据编码成纯文本形式，以便在网页中直接嵌入或者在数据传输中以文本的形式进行传递。在前端开发中，可以将图片转换为Base64编码以减少HTTP请求次数，从而提高网页加载速度。

例如，可以使用以下Python代码将图片转换为Base64编码：

```python
import base64

with open("image.jpg", "rb") as image_file:
    base64_image = base64.b64encode(image_file.read()).decode("utf-8")

print(base64_image)
```

在这个示例中，"image.jpg" 是图片文件的路径。`b64encode()`函数将图片的二进制数据编码为Base64编码，然后使用`.decode("utf-8")`将字节串转换为字符串形式。

在前端开发中，可以直接使用HTML的`<img>`标签，并将Base64编码的图片数据直接嵌入到`src`属性中，如下所示：

```html
<img src="data:image/jpeg;base64,<base64_image_data_here>" alt="Image">
```

其中 `<base64_image_data_here>` 是图片的Base64编码数据。

作用，缘由，做什么，为什么要做

## python

### requests


### xpath/lxml,bs4,re


### 协程







## 浏览器开发者工具





#### 开发者工具检测



#### 打开开发者工具页面重定向/关闭





## 自动化/rpc
### Selenium





### 验证码识别

数字验证码

计算题验证码

轨迹【画一个对号等】

旋转【旋转圆形风景图片为正】

滑块【拖动小图形填充进缺口】



无感验证码




## JS逆向-Hook





## JS逆向-加密算法





### md5

### aes



## webpack





## 混淆





### 方法与目标



#### 变量名混淆

减少代码可读性，隐藏掉语义化的变量名，函数名







### 工具



## 浏览器指纹



## 蜜罐



## 风控







## 脱混淆/ast





## wasm





## 简历写法，面试技巧







## 副业接单经验





# 借鉴

### 专栏大纲：爬虫从入门到精通

#### 第1部分：爬虫基础
1. **介绍网络爬虫**
   - 爬虫的定义和主要功能
   - 爬虫在业界的应用实例
   - 法律和道德问题：遵守robots.txt协议

2. **HTML/CSS/JavaScript基础**
   - 理解网页的结构
   - 常用的HTML标签和属性
   - CSS选择器的使用
   - JavaScript在网页中的作用

3. **开发环境设置**
   - Python基础
   - 安装Python和相关库（如requests, BeautifulSoup, Selenium）
   - 使用IDE和其他工具的介绍

4. **初级爬虫项目**
   - 使用requests获取网页内容
   - BeautifulSoup入门：解析HTML
   - 简单的数据提取实例

#### 第2部分：进阶爬虫技术
1. **处理JavaScript渲染的页面**
   - 理解浏览器渲染过程
   - 使用Selenium模拟浏览器操作
   - 处理动态加载数据

2. **数据存储方案**
   - 保存数据到文本文件
   - 使用数据库：SQL与NoSQL选型和使用
   - 数据清洗和格式化

3. **避免爬虫被封**
   - 用户代理（User-Agent）和IP轮换技术
   - 理解频率限制及其规避策略
   - 使用代理服务器

4. **中级爬虫项目**
   - 爬取电子商务网站的产品信息
   - 实现一个天气数据爬虫
   - 新闻网站的内容抓取和存储

#### 第3部分：高级爬虫开发
1. **爬虫框架Scrapy**
   - Scrapy框架的架构和优势
   - 创建Scrapy项目和爬虫
   - 数据提取和管道处理

2. **分布式爬虫**
   - 分布式爬虫的原理和实现
   - 使用Scrapy-Redis实现分布式爬虫

3. **爬虫的监控和维护**
   - 监控爬虫的运行状态
   - 日志记录和错误处理
   - 爬虫的维护和更新策略

4. **高级爬虫项目**
   - 社交媒体数据的自动抓取和分析
   - 实时航班信息爬虫
   - 大规模电商网站的定期数据抓取

#### 第4部分：专栏总结
1. **爬虫项目的最佳实践**
2. **未来爬虫技术的发展趋势**
3. **如何将爬虫技能转化为职业优势**

这个大纲为读者提供了从基础知识到高级技能的全面学习路径，不仅帮助新手快速入门，还能让有经验的开发者进一步提高他们的技术水平。每部分结束后可以包括一个实际项目，使读者能够通过实践进一步巩固和深化所学知识。





